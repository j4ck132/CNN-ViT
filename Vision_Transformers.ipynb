{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import wandb\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision as tv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "Ic1Itnb-SrBf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GLUActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return 0.5 * x * (1.0 + torch.erf(x / math.sqrt(2.0)))"
      ],
      "metadata": {
        "id": "1jD1xCI9ZzuX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "-c0-FdkzPywX"
      },
      "outputs": [],
      "source": [
        "class multiHeadAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, num_attention_heads, dropout_rate, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        self.num_attention_heads = num_attention_heads\n",
        "        self.attention_head_size = hidden_size // num_attention_heads\n",
        "        self.all_head_size = hidden_size\n",
        "\n",
        "        self.qkv = nn.Linear(hidden_size, self.all_head_size * 3, bias=qkv_bias)\n",
        "        self.attn_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.out_proj = nn.Linear(hidden_size, hidden_size)\n",
        "        self.proj_dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, query, key, value):\n",
        "        B, N, _ = query.shape\n",
        "        qkv = (self.qkv(query)\n",
        "               .reshape(B, N, 3, self.num_attention_heads, self.attention_head_size)\n",
        "               .permute(2, 0, 3, 1, 4))\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn_scores = (q @ k.transpose(-2, -1)) / math.sqrt(self.attention_head_size)\n",
        "        attn_probs = F.softmax(attn_scores, dim=-1)\n",
        "        attn_probs = self.attn_dropout(attn_probs)\n",
        "\n",
        "        context = (attn_probs @ v).transpose(1, 2).reshape(B, N, self.all_head_size)\n",
        "        out = self.out_proj(context)\n",
        "        out = self.proj_dropout(out)\n",
        "        return out, attn_probs"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, hidden_size, intermediate_size, dropout_rate):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(hidden_size, intermediate_size)\n",
        "        self.activation = GLUActivation()\n",
        "        self.dropout1 = nn.Dropout(dropout_rate)\n",
        "        self.fc2 = nn.Linear(intermediate_size, hidden_size)\n",
        "        self.dropout2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "cT3YlZiiTBli"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, hidden_size, num_attention_heads, intermediate_size, dropout_rate, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.attn = multiHeadAttention(hidden_size, num_attention_heads, dropout_rate, qkv_bias=qkv_bias)\n",
        "        self.norm2 = nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.mlp = MLP(hidden_size, intermediate_size, dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x_norm = self.norm1(x)\n",
        "        attn_out, _ = self.attn(x_norm, x_norm, x_norm)\n",
        "        x = h + attn_out\n",
        "\n",
        "        h2 = x\n",
        "        x_norm2 = self.norm2(x)\n",
        "        mlp_out = self.mlp(x_norm2)\n",
        "        x = h2 + mlp_out\n",
        "        return x"
      ],
      "metadata": {
        "id": "9io-1oyNafUZ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, image_size=32, patch_size=4, num_channels=3, num_classes=10,\n",
        "                 hidden_size=48, num_hidden_layers=4, num_attention_heads=4,\n",
        "                 intermediate_size=192, dropout_rate=0.1, qkv_bias=True):\n",
        "        super().__init__()\n",
        "        assert image_size % patch_size == 0, \"Image must be divisible by patch size!\"\n",
        "        num_patches = (image_size // patch_size) ** 2\n",
        "\n",
        "        self.projection = nn.Conv2d(in_channels=num_channels, out_channels=hidden_size,\n",
        "                                    kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, hidden_size))\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, hidden_size))\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.encoder_blocks = nn.ModuleList([\n",
        "            TransformerEncoderBlock(\n",
        "                hidden_size=hidden_size,\n",
        "                num_attention_heads=num_attention_heads,\n",
        "                intermediate_size=intermediate_size,\n",
        "                dropout_rate=dropout_rate,\n",
        "                qkv_bias=qkv_bias\n",
        "            ) for _ in range(num_hidden_layers)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(hidden_size, eps=1e-6)\n",
        "        self.classifier = nn.Linear(hidden_size, num_classes)\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                nn.init.trunc_normal_(module.weight, std=0.02)\n",
        "                if module.bias is not None:\n",
        "                    nn.init.zeros_(module.bias)\n",
        "            elif isinstance(module, nn.LayerNorm):\n",
        "                nn.init.zeros_(module.bias)\n",
        "                nn.init.ones_(module.weight)\n",
        "        nn.init.trunc_normal_(self.position_embeddings, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        x = x + self.position_embeddings\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # pass through encoder blocks\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        logits = self.classifier(x[:, 0])\n",
        "        return logits"
      ],
      "metadata": {
        "id": "OzNlukFTa0LI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR100Data:\n",
        "    def __init__(self, batch_size=64, resize=(32, 32), root=\"./data\"):\n",
        "        self.batch_size = batch_size\n",
        "        self.resize = resize\n",
        "        self.root = root\n",
        "\n",
        "        # CIFAR-100 normalization constants\n",
        "        mean = (0.5071, 0.4867, 0.4408)\n",
        "        std = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "        # train transforms: augment\n",
        "        self.train_transform = transforms.Compose([\n",
        "            transforms.RandomCrop(resize[0], padding=4),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            transforms.ColorJitter(0.1, 0.1, 0.1, 0.1),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "        # val transforms: only resize + normalize\n",
        "        self.val_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean, std),\n",
        "        ])\n",
        "\n",
        "        self.train = tv.datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=True,\n",
        "            transform=self.train_transform,\n",
        "            download=True,\n",
        "        )\n",
        "        self.val = tv.datasets.CIFAR100(\n",
        "            root=self.root,\n",
        "            train=False,\n",
        "            transform=self.val_transform,\n",
        "            download=True,\n",
        "        )\n",
        "        self.classes = self.train.classes\n",
        "\n",
        "    def get_dataloader(self, train=True):\n",
        "        dataset = self.train if train else self.val\n",
        "        return DataLoader(dataset, self.batch_size, shuffle=train, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "b-HDoTagfJRt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FashionMNIST(nn.Module):\n",
        "\n",
        "  def __init__(self, batch_size = 64, resize = (28, 28), root = './data'):\n",
        "    super().__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.resize = resize\n",
        "    self.root = root\n",
        "\n",
        "    # data augmentation via color jitter and flip\n",
        "    color_aug = tv.transforms.ColorJitter(brightness = 0.25, contrast = 0.25, saturation = 0.25, hue = 0.25)\n",
        "    train_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.RandomHorizontalFlip(),\n",
        "            color_aug,\n",
        "            transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # no data augmentation for validation\n",
        "    val_transform = transforms.Compose([\n",
        "            transforms.Resize(resize),\n",
        "            transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    # access datasets within torchvision\n",
        "    self.train = tv.datasets.FashionMNIST(root=self.root, train=True , transform=train_transform, download=True)\n",
        "    self.val   = tv.datasets.FashionMNIST(root=self.root, train=False, transform=val_transform  , download=True)\n",
        "\n",
        "  def text_labels(self, indices):\n",
        "    labels = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
        "\n",
        "    return [labels[i] for i in indices]\n",
        "\n",
        "  def get_dataloader(self, train):\n",
        "    data = self.train if train else self.val\n",
        "\n",
        "    # data-iterator reads mini-batch of data\n",
        "    # key component for efficient performance\n",
        "    # exploit high-performance cmputing to avoid slowing down training loop\n",
        "    return torch.utils.data.DataLoader(data, self.batch_size, shuffle = train)\n",
        "\n",
        "  def train_dataloader(self):\n",
        "    return self.get_dataloader(train = True)"
      ],
      "metadata": {
        "id": "lE6da0sZywwU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_attention_maps(\n",
        "    model: VisionTransformer,\n",
        "    images: torch.Tensor,\n",
        "    attn_maps: list,\n",
        "    patch_size: int,\n",
        "    device: torch.device,\n",
        "    n_heads_to_show: int = 4,\n",
        "):\n",
        "    \"\"\"\n",
        "    images: [B, C, H, W]\n",
        "    attn_maps: list of tensors [B, heads, N, N]\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    B, C, H, W = images.shape\n",
        "\n",
        "    # pick first image\n",
        "    img = images[0].cpu().permute(1,2,0).numpy()\n",
        "    fig, axes = plt.subplots(1 + n_heads_to_show, 1 + n_heads_to_show, figsize=(8, 8))\n",
        "\n",
        "    # show original image\n",
        "    axes[0,0].imshow((img - img.min())/(img.max()-img.min()), interpolation='nearest')\n",
        "    axes[0,0].set_title('Input')\n",
        "    axes[0,0].axis('off')\n",
        "\n",
        "    # for each head in first layer\n",
        "    first_attn = attn_maps[0][0]  # [heads, N, N]\n",
        "    num_patches = int((H//patch_size) * (W//patch_size))\n",
        "    for h in range(min(n_heads_to_show, first_attn.shape[0])):\n",
        "        attn_head = first_attn[h, 0, 1:]  # cls token -> patches\n",
        "        attn_map = attn_head.reshape(H//patch_size, W//patch_size).detach().cpu().numpy()\n",
        "        attn_map = attn_map / attn_map.max()\n",
        "\n",
        "        # upsample\n",
        "        attn_map = torch.tensor(attn_map).unsqueeze(0).unsqueeze(0)\n",
        "        attn_map = F.interpolate(attn_map, size=(H, W), mode='bilinear', align_corners=False).squeeze().numpy()\n",
        "\n",
        "        # overlay\n",
        "        axes[0, h+1].imshow((img - img.min())/(img.max()-img.min()), interpolation='nearest')\n",
        "        axes[0, h+1].imshow(attn_map, alpha=0.5, interpolation='nearest')\n",
        "        axes[0, h+1].set_title(f'Head {h}')\n",
        "        axes[0, h+1].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "XKzoU3zrYrOt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_embedding_filters(model, n_components=28):\n",
        "    \"\"\"\n",
        "    1) RGB embedding filters (first 28 principal components)\n",
        "    \"\"\"\n",
        "    # extract the patch-projection conv2d weights:\n",
        "    # shape = [hidden_size, channels, patch_size, patch_size]\n",
        "    w = model.projection.weight.data.cpu().numpy()\n",
        "    n_filters, C, p, _ = w.shape\n",
        "\n",
        "    # flatten each filter to a vector\n",
        "    w_flat = w.reshape(n_filters, -1)  # [hidden, C*p*p]\n",
        "\n",
        "    # run PCA on these filters and grab the top components\n",
        "    max_comp = min(n_filters, w_flat.shape[1])\n",
        "    n_comp   = min(n_components, max_comp)\n",
        "    pca = PCA(n_components=n_comp)\n",
        "    pca.fit(w_flat)\n",
        "    pcs = pca.components_         # [n_components, C*p*p]\n",
        "    pcs = pcs.reshape(n_comp, C, p, p)\n",
        "\n",
        "    # plot in a grid (here 4×7)\n",
        "    cols = 7\n",
        "    rows = int(np.ceil(n_comp/cols))\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(cols, rows))\n",
        "    for i in range(n_comp):\n",
        "        ax = axes[i//cols, i%cols]\n",
        "        # normalize and transpose to H×W×C\n",
        "        img = pcs[i].transpose(1,2,0)\n",
        "        img = (img - img.min())/(img.max()-img.min())\n",
        "        ax.imshow(img, interpolation='nearest')\n",
        "        ax.axis('off')\n",
        "    plt.suptitle('RGB embedding filters (first 28 principal components)', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_position_embedding_similarity(model):\n",
        "    \"\"\"\n",
        "    2) Position embedding similarity: one heatmap per patch, arranged in the patch grid.\n",
        "    \"\"\"\n",
        "    # grab learnable position embeddings (drop the CLS token)\n",
        "    pos = model.position_embeddings.data.cpu().numpy().squeeze(0)[1:,:]  # [N, hidden]\n",
        "    N, _ = pos.shape\n",
        "    grid = int(np.sqrt(N))\n",
        "\n",
        "    # cosine similarity matrix\n",
        "    norms = np.linalg.norm(pos, axis=1, keepdims=True)\n",
        "    cos_sim = (pos @ pos.T) / (norms @ norms.T)   # [N, N]\n",
        "\n",
        "    fig, axes = plt.subplots(grid, grid, figsize=(grid, grid))\n",
        "    for i in range(N):\n",
        "        ax = axes[i//grid, i%grid]\n",
        "        tile = cos_sim[i].reshape(grid, grid)\n",
        "        im = ax.imshow(tile, interpolation='nearest', vmin=-1, vmax=1)\n",
        "        ax.axis('off')\n",
        "\n",
        "    # shared colorbar\n",
        "    fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.6)\n",
        "    plt.suptitle('Position embedding similarity', y=1.02)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_mean_attention_distance(attn_maps, patch_size, image_height, image_width):\n",
        "    \"\"\"\n",
        "    3) Scatter of mean CLS→patch attention distance vs. layer depth.\n",
        "       - attn_maps: list of length=L of tensors [B, heads, N, N]\n",
        "       - patch_size: pixel size of one patch\n",
        "       - image_height/width: original image H, W\n",
        "    \"\"\"\n",
        "    # build patch‐center coords\n",
        "    # N includes the CLS token, so index 0 is CLS; the remaining N-1 are patches\n",
        "    _, heads, N, _ = attn_maps[0].shape\n",
        "    P = int(np.sqrt(N-1))\n",
        "    coords = []\n",
        "    for y in range(P):\n",
        "        for x in range(P):\n",
        "            coords.append(np.array([(y+0.5)*patch_size, (x+0.5)*patch_size]))\n",
        "    coords = np.stack(coords)   # [N-1, 2]\n",
        "\n",
        "    # assume CLS token at the top-left (0,0) patch center\n",
        "    cls_coord = np.array([patch_size/2, patch_size/2])\n",
        "\n",
        "    xs, ys, cs = [], [], []\n",
        "    for layer_idx, layer_attn in enumerate(attn_maps):\n",
        "        # pick batch=0, then for each head take CLS→patch (row 0, cols 1:])\n",
        "        arr = layer_attn[0].cpu().numpy()  # [heads, N, N]\n",
        "        for h in range(heads):\n",
        "            w = arr[h, 0, 1:]              # [N-1]\n",
        "            dists = np.linalg.norm(coords - cls_coord, axis=1)\n",
        "            mean_dist = (w * dists).sum() / w.sum()\n",
        "            xs.append(layer_idx)\n",
        "            ys.append(mean_dist)\n",
        "            cs.append(h)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    scatter = ax.scatter(xs, ys, c=cs)\n",
        "    handles, labels = scatter.legend_elements()\n",
        "    ax.legend(handles, [f\"Head {i}\" for i in range(heads)], title=\"Attention Heads\")\n",
        "    ax.set_xlabel(\"Network depth (layer)\")\n",
        "    ax.set_ylabel(\"Mean attention distance (pixels)\")\n",
        "    ax.set_title(\"ViT Mean CLS→patch attention distance\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def get_attention_maps(model: VisionTransformer, images: torch.Tensor):\n",
        "  attn_maps = []\n",
        "  hooks = []\n",
        "  for block in model.encoder_blocks:\n",
        "      # each block.attn returns (out, attn_probs)\n",
        "      hooks.append(\n",
        "          block.attn.register_forward_hook(\n",
        "              lambda module, inp, out: attn_maps.append(out[1].detach().cpu())\n",
        "          )\n",
        "      )\n",
        "  # run a single forward pass to fill attn_maps\n",
        "  _ = model(images)\n",
        "  # remove hooks\n",
        "  for h in hooks:\n",
        "      h.remove()\n",
        "  return attn_maps"
      ],
      "metadata": {
        "id": "3dWb3jlsxoIx"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_experiment(\n",
        "    dataset_name: str,\n",
        "    patch_size: int,\n",
        "    hidden_size: int,\n",
        "    num_layers: int,\n",
        "    num_heads: int,\n",
        "    dropout_rate: float,\n",
        "    lr: float,\n",
        "    batch_size: int = 128,\n",
        "    max_epochs: int = 25,\n",
        "):\n",
        "    # 2) initalizing wandb run\n",
        "    wandb.init(\n",
        "        project=\"ViT-Experiments-Graphs\",\n",
        "        name=f\"{dataset_name}_ps{patch_size}_hs{hidden_size}_nl{num_layers}_lr{lr}\",\n",
        "        config={\n",
        "            \"dataset\": dataset_name,\n",
        "            \"patch_size\": patch_size,\n",
        "            \"hidden_size\": hidden_size,\n",
        "            \"num_layers\": num_layers,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"dropout_rate\": dropout_rate,\n",
        "            \"lr\": lr,\n",
        "            \"batch_size\": batch_size,\n",
        "            \"max_epochs\": max_epochs,\n",
        "        }\n",
        "    )\n",
        "    config = wandb.config\n",
        "\n",
        "    # prepping data, though resize is not really needed\n",
        "    resize = (config.patch_size * int(32/config.patch_size),\n",
        "              config.patch_size * int(32/config.patch_size))\n",
        "    if config.dataset == \"FashionMNIST\":\n",
        "        data_module = FashionMNIST(batch_size=config.batch_size,\n",
        "                                   resize=resize)\n",
        "        num_channels, num_classes = 1, 10\n",
        "        train_loader = data_module.train_dataloader()\n",
        "        val_loader   = data_module.get_dataloader(train=False)\n",
        "    else:\n",
        "        data_module = CIFAR100Data(batch_size=config.batch_size,\n",
        "                                         resize=resize)\n",
        "        num_channels, num_classes = 3, 100\n",
        "        train_loader = data_module.get_dataloader(train=True)\n",
        "        val_loader   = data_module.get_dataloader(train=False)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    model = VisionTransformer(\n",
        "        image_size=resize[0],\n",
        "        patch_size=config.patch_size,\n",
        "        num_channels=num_channels,\n",
        "        num_classes=num_classes,\n",
        "        hidden_size=config.hidden_size,\n",
        "        num_hidden_layers=config.num_layers,\n",
        "        num_attention_heads=config.num_heads,\n",
        "        intermediate_size=config.hidden_size * 4,\n",
        "        dropout_rate=config.dropout_rate,\n",
        "        qkv_bias=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Adam optimizer, loss, cosine scheduler\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "    optimizer = optim.AdamW(model.parameters(),\n",
        "                            lr=config.lr,\n",
        "                            weight_decay=0.05)\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer,\n",
        "                                                     T_max=config.max_epochs)\n",
        "\n",
        "    # training\n",
        "    for epoch in range(1, config.max_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(images)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            preds = logits.argmax(dim=1)\n",
        "            correct_train += (preds == labels).sum().item()\n",
        "            total_train += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "        train_loss = total_loss / len(train_loader)\n",
        "        train_acc = correct_train / total_train\n",
        "\n",
        "        # validate\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                logits = model(images)\n",
        "                loss = criterion(logits, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "                preds = logits.argmax(dim=1)\n",
        "                correct_val += (preds == labels).sum().item()\n",
        "                total_val += labels.size(0)\n",
        "\n",
        "                all_preds.append(preds.cpu())\n",
        "                all_labels.append(labels.cpu())\n",
        "        val_loss = total_val_loss / len(val_loader)\n",
        "        val_acc = correct_val / total_val\n",
        "        all_preds = torch.cat(all_preds).numpy()\n",
        "        all_labels = torch.cat(all_labels).numpy()\n",
        "\n",
        "        precision = precision_score(all_labels, all_preds, average='macro')\n",
        "        recall = recall_score(all_labels, all_preds, average='macro')\n",
        "        f1 = f1_score(all_labels, all_preds, average='macro')\n",
        "\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_acc\": train_acc,\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_acc\": val_acc,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1,\n",
        "        })\n",
        "        scheduler.step()\n",
        "\n",
        "    # save model weights\n",
        "    os.makedirs('weights', exist_ok=True)\n",
        "    weight_path = f\"weights/{dataset_name}_ps{patch_size}_hs{hidden_size}_nl{num_layers}_lr{lr}.pth\"\n",
        "    torch.save(model.state_dict(), weight_path)\n",
        "    wandb.save(weight_path)\n",
        "\n",
        "    images, _ = next(iter(val_loader))\n",
        "    images = images.to(device)\n",
        "\n",
        "    # 1, PCA of patch-embedding filters\n",
        "    plot_embedding_filters(model, n_components=28)\n",
        "\n",
        "    # 2, position embedding similarity\n",
        "    plot_position_embedding_similarity(model)\n",
        "\n",
        "    # 3, mean CLS→patch attention distance\n",
        "    attn_maps = get_attention_maps(model, images)\n",
        "    plot_mean_attention_distance(\n",
        "        attn_maps,\n",
        "        patch_size=config.patch_size,\n",
        "        image_height=resize[0],\n",
        "        image_width=resize[1]\n",
        "    )\n",
        "\n",
        "    wandb.finish()\n",
        "\n",
        "# final hyperparameter sweep\n",
        "if __name__ == \"__main__\":\n",
        "    patch_sizes = [7]\n",
        "    hidden_sizes = [48]\n",
        "    learning_rates = [1e-3]\n",
        "    fixed_num_layers = 4\n",
        "    dropout_rate = 0.1\n",
        "    num_heads = 4\n",
        "\n",
        "    for ps in patch_sizes:\n",
        "        for hs in hidden_sizes:\n",
        "          for lr in learning_rates:\n",
        "              run_experiment(\n",
        "                  dataset_name=\"FashionMNIST\",\n",
        "                  patch_size=ps,\n",
        "                  hidden_size=hs,\n",
        "                  num_layers=fixed_num_layers,\n",
        "                  num_heads=num_heads,\n",
        "                  dropout_rate=dropout_rate,\n",
        "                  lr=lr,\n",
        "                  batch_size=128,\n",
        "                  max_epochs=40\n",
        "              )\n",
        "\n",
        "\n",
        "    patch_sizes = [4]\n",
        "    hidden_sizes = [128]\n",
        "    learning_rates = [5e-4]\n",
        "    fixed_num_layers = 12\n",
        "    dropout_rate = 0.1\n",
        "    num_heads = 8\n",
        "\n",
        "    for ps in patch_sizes:\n",
        "            for hs in hidden_sizes:\n",
        "              for lr in learning_rates:\n",
        "                  run_experiment(\n",
        "                      dataset_name=\"CIFAR100Data\",\n",
        "                      patch_size=ps,\n",
        "                      hidden_size=hs,\n",
        "                      num_layers=fixed_num_layers,\n",
        "                      num_heads=num_heads,\n",
        "                      dropout_rate=dropout_rate,\n",
        "                      lr=lr,\n",
        "                      batch_size=128,\n",
        "                      max_epochs=200\n",
        "                  )"
      ],
      "metadata": {
        "id": "I0rmTCGAg0QH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
